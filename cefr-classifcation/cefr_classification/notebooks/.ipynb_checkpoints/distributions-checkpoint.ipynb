{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6f73b3c-7e6e-4ce6-bb3a-f995df38305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5e1964f-8ad0-421b-9b48-da41b91849a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/berstearns/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/berstearns/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/berstearns/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de672a23-0070-4cd2-95ba-160147c8eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from nltk.corpus import treebank\n",
    ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    ">>> t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e91c59-93b5-4b7a-9e06-81f937fa0d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/home/berstearns/p/garbage_collector/cefr-classifcation/cefr_classification/venv/lib/python3.12/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec3ebae-cc2d-464d-a0e3-e102e597bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "base_dir = os.path.dirname(notebook_dir)\n",
    "celva_data_folder = os.path.join(base_dir,\"datasets\", \"CELVA\")\n",
    "celva_dataset_fp = os.path.join(celva_data_folder, \"celva.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "806d289e-5c91-47bb-ac84-0772248f635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, TypeVar, List, Callable\n",
    "class CleaningPipeline:\n",
    "    def __init__(self):\n",
    "        # Define mappings for replacements\n",
    "        self.whitespace_map = {c: \" \" for c in string.whitespace}\n",
    "        self.replaces = {\n",
    "            \"...\": \"\",\n",
    "            \"..\": \"\",\n",
    "        }\n",
    "        self.specials = string.whitespace + \".()\"\n",
    "\n",
    "    def __call__(self, input_data: Union[str, List[str], pd.DataFrame], column_name: str = None):\n",
    "        \"\"\"\n",
    "        Choose between single text, list of texts, or DataFrame processing.\n",
    "        \"\"\"\n",
    "        if isinstance(input_data, str):\n",
    "            # Single text processing\n",
    "            return self.clean_text(input_data)\n",
    "        elif isinstance(input_data, list):\n",
    "            # List of texts processing\n",
    "            return [self.clean_text(text) for text in input_data]\n",
    "        elif isinstance(input_data, pd.DataFrame):\n",
    "            # DataFrame processing\n",
    "            if column_name is None:\n",
    "                raise ValueError(\"column_name must be provided when input_data is a DataFrame.\")\n",
    "            return self.clean_dataframe(input_data, column_name)\n",
    "        else:\n",
    "            raise TypeError(\"Input type not supported. Must be str, list, or pandas DataFrame.\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean a single text string.\n",
    "        \"\"\"\n",
    "        # Normalize whitespace and lowercase\n",
    "        text = \"\".join([self.whitespace_map.get(ch.lower(), ch.lower()) for ch in text if ch.isalnum() or ch in self.specials])\n",
    "        \n",
    "        # Replace patterns like \"...\", \"..\"\n",
    "        for pattern, replacement in self.replaces.items():\n",
    "            text = text.replace(pattern, replacement)\n",
    "        \n",
    "        # Remove multiple dots and spaces\n",
    "        text = re.sub(r'\\.{2,}', '', text)\n",
    "        text = re.sub(r' {2,}', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def clean_dataframe(self, df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean a text column in a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "        \n",
    "        # Apply cleaning to the specified column\n",
    "        df[column_name] = df[column_name].apply(self.clean_text)\n",
    "        return df\n",
    "\n",
    "class Text2SentencePipeline:\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]):\n",
    "        \"\"\"\n",
    "        Initialize the Text2Sentence class with a tokenizer function.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: A function that takes a string and returns a list of sentences.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def text2sentence(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize a single text into sentences using the provided tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(text)\n",
    "\n",
    "    def __call__(self, input_data: Union[str, List[str], pd.DataFrame], column_name: str = None) -> Union[List[str], List[List[str]], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Choose between single text, list of texts, or DataFrame processing.\n",
    "\n",
    "        Args:\n",
    "            input_data: A string, list of strings, or pandas DataFrame.\n",
    "            column_name: The name of the column to process if input_data is a DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            Tokenized sentences based on the input type.\n",
    "        \"\"\"\n",
    "        if isinstance(input_data, str):\n",
    "            # Single text processing\n",
    "            return self.text2sentence(input_data)\n",
    "        elif isinstance(input_data, list):\n",
    "            # List of texts processing\n",
    "            return [self.text2sentence(text) for text in input_data]\n",
    "        elif isinstance(input_data, pd.DataFrame):\n",
    "            # DataFrame processing\n",
    "            if column_name is None:\n",
    "                raise ValueError(\"column_name must be provided when input_data is a DataFrame.\")\n",
    "            df = input_data.copy()\n",
    "            df[column_name] = df[column_name].apply(self.text2sentence)\n",
    "            return df\n",
    "        else:\n",
    "            raise TypeError(\"Input type not supported. Must be str, list, or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb42223-0308-42ab-91a9-05193d38ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(celva_dataset_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49dd499f-59b3-4104-b6d1-0acae6ac0cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame with multiple columns to the single column sentences",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12238/3905787493.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCleaningPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext2sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2SentencePipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cleaned_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Texte_etudiant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Texte_etudiant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext2sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cleaned_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p/garbage_collector/cefr-classifcation/cefr_classification/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4297\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4298\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4300\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4302\u001b[0m         elif (\n\u001b[1;32m   4303\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4304\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p/garbage_collector/cefr-classifcation/cefr_classification/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4456\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4459\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   4460\u001b[0m                 \u001b[0;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4461\u001b[0m                 \u001b[0;34mf\"\u001b[0m\u001b[0;34mcolumn \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4462\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set a DataFrame with multiple columns to the single column sentences"
     ]
    }
   ],
   "source": [
    "clean = CleaningPipeline()\n",
    "text2sentence = Text2SentencePipeline(tokenizer=nltk.tokenize.sent_tokenize)\n",
    "df[\"cleaned_text\"] = clean(df, column_name=\"Texte_etudiant\")[\"Texte_etudiant\"]\n",
    "df[\"sentences\"] = text2sentence(df, column_name=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9762f1c6-ab88-45a1-b3fc-888514fe65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['a earth sciences domain', 'the beginning', 'the year', 'i', 'a little city', 'paris un', 'the south', 'paris', 'a few classes', 'cartographic basics', 'it', 'time', 'us', 'it', 'real life', 'what', 'it', 'the field', 'the aim', 'this trip', 'a couple', 'series', 'rock', 'which ones', 'which period', 'the earth time', 'everything', 'we', 'a map', 'a geological map', 'a place', 'we', '10 days', 'we', 'the fauna', 'the wind', 'the sun', 'the rain', 'around 5 to 10 kilometers', 'day', '6pm', 'the field', 'rocks', 'we', 'all', 'us', 'geosciences students', 'little groups', '2 people', 'my partner', 'i', 'attention', 'all the rocks', 'us', 'a in place serie', 'we', 'a pendage', 'which', 'the angle', 'the serie', 'the geographical north pole', 'all those pendages', 'we', 'the north', 'the field', 'the average center', 'the series', 'a syncline', 'our first element', 'our map', 'an syncline', 'all the younger rocks', 'the center', 'we', 'the south', 'the field', 'it', 'us debutants', 'cartographics', 'our map', 'we', 'the south', 'discordance', 'which', 'news deposits', 'the olders rocks', 'all', 'that', 'we', 'our homes', 'what', 'the south']\n",
      "Verbs: ['go', 'call', 'do', 'see', 'recognize', 'identify', 'belong', 'report', 'see', 'create', 'have', 'wake', 'walk', 'climb', 'walk', 'study', 'watch', 'pay', 'be', 'have', 'take', 'mean', 'direct', 'compare', 'take', 'notice', 'make', 'here', 'go', 'try', 'understand', 'fail', 'finish', 'learn', 'mean', 'place', 'erode', 'go', 'walk', 'climb', 'think', 'happen']\n",
      "the beginning of the year DATE\n",
      "paris GPE\n",
      "paris GPE\n",
      "10 days DATE\n",
      "every morning TIME\n",
      "7 CARDINAL\n",
      "8 CARDINAL\n",
      "around 5 CARDINAL\n",
      "6pm TIME\n",
      "2 CARDINAL\n",
      "first ORDINAL\n",
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de9133-54fe-4c45-81dc-bd19f06dc0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed629792-02e1-4992-a0e6-f3d997704dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['being in a earth sciences domain at the beginning of the year my class and i went to a little city called paris un the south of paris.',\n",
       " 'after a few classes of cartographic basics it was time for us to do it in real life to see what it is like to be on the field.',\n",
       " 'the aim of this trip was to recognize a couple of series of rock and identify which ones belongs to which period of the earth time.',\n",
       " 'and then being able to report everything we saw on a map and create a geological map of a place weve never been before.',\n",
       " 'for 10 days we had to wake up every morning a 7 to be ready at 8 to walk through the fauna and flora climbing mountains against the wind or the sun or the rain and walk around 5 to 10 kilometers per day until 6pm.',\n",
       " 'the field was around 12m2 and rocks we studied about were sedimentary.',\n",
       " 'all of us geosciences students were separate into little groups of 2 people.',\n",
       " 'my partner and i were watching out and paying attention to all the rocks around us.',\n",
       " 'when there is a in place serie we had to take a pendage which means the angle of where the serie is directed compared the geographical north pole.',\n",
       " 'by taking all those pendages we noticed that from the north of the field to the average center the series were making a syncline.',\n",
       " 'heres our first element of our map an syncline with all the younger rocks in the center and symmetrically going to the oldest.',\n",
       " 'we tried to understand the south of the field but failed it was too complicated for us debutants in cartographics.',\n",
       " 'after finishing our map we learnt that the south were in discordance which means news deposits were placed into the olders rocks eroded.',\n",
       " 'after all of that we went back to our homes tired of walking and climbing and thinking about what happened in the south.',\n",
       " 'cool']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eac71a-1d2b-4877-8fc8-5b7d352fc283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676bfe9-201c-4748-8fe4-110b4c16384f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f110c-dd32-4602-ba29-21bd6235ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce8366-35a0-4d42-b9ec-3fe48d01f2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd51d7-e54f-4854-8d8d-51a46e13e4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f658419-fedb-4684-8e36-a29c26a2b5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6efcffc0-01c1-4722-b016-7f0034ab25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      " being in a earth sciences domain at the beginning of the year my class and i went to a little city called paris un the south of paris. after a few classes of cartographic basics it was time for us to do it in real life to see what it is like to be on the field. the aim of this trip was to recognize a couple of series of rock and identify which ones belongs to which period of the earth time. and then being able to report everything we saw on a map and create a geological map of a place weve never been before. for 10 days we had to wake up every morning a 7 to be ready at 8 to walk through the fauna and flora climbing mountains against the wind or the sun or the rain and walk around 5 to 10 kilometers per day until 6pm. the field was around 12m2 and rocks we studied about were sedimentary. all of us geosciences students were separate into little groups of 2 people. my partner and i were watching out and paying attention to all the rocks around us. when there is a in place serie we had to take a pendage which means the angle of where the serie is directed compared the geographical north pole. by taking all those pendages we noticed that from the north of the field to the average center the series were making a syncline. heres our first element of our map an syncline with all the younger rocks in the center and symmetrically going to the oldest. we tried to understand the south of the field but failed it was too complicated for us debutants in cartographics. after finishing our map we learnt that the south were in discordance which means news deposits were placed into the olders rocks eroded. after all of that we went back to our homes tired of walking and climbing and thinking about what happened in the south. cool \n",
      "\n",
      "\n",
      "sent: [being in a earth sciences domain at the beginning of the year my class, and i went to a little city called paris un the south of paris., after a few classes of cartographic basics it was time for us to do it in real life to see what it is like to be on the field.]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = df[\"cleaned_text\"][0]\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "sents = [s for s in doc.sents]\n",
    "print(dir(doc))\n",
    "print(f\"text:\\n {text}\",\"\\n\\n\")\n",
    "print(f\"sent: {sents[0:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77af343-2867-4b9b-bb5b-cbe396a92358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a57bc-25c5-4d54-afe0-ddfedd42ee60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917362c0-d4dd-45bb-9631-65954257ee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea173f9b-6198-457a-a0f3-55b4463099e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cccffb-79cd-4c46-9676-9860e2aae842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dfa74-2a8d-464d-96f7-29ea49353d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d707de-59a1-4ccd-89e4-455f76648412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4aa80-7ca8-4a99-81ed-42512fbb6a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32f64c-659b-4d7f-ad75-e91aaec23b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e6f54-6ac0-440c-805c-4d7020d012b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcc1f0-df52-4a8a-a77b-ce1d36bc4dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c9e103-77f1-4d16-8373-e5103f6f1b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_annees_L2</th>\n",
       "      <th>Sejours_duree_semaines</th>\n",
       "      <th>Sejours_frequence</th>\n",
       "      <th>Lang_exposition</th>\n",
       "      <th>Section_renforcee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1045.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.495694</td>\n",
       "      <td>3.543541</td>\n",
       "      <td>2.027751</td>\n",
       "      <td>5.623541</td>\n",
       "      <td>0.371292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.498128</td>\n",
       "      <td>17.252610</td>\n",
       "      <td>14.857374</td>\n",
       "      <td>9.175903</td>\n",
       "      <td>0.483382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nb_annees_L2  Sejours_duree_semaines  Sejours_frequence  \\\n",
       "count   1045.000000             1045.000000        1045.000000   \n",
       "mean       9.495694                3.543541           2.027751   \n",
       "std        2.498128               17.252610          14.857374   \n",
       "min        0.000000                0.000000           0.000000   \n",
       "25%        8.000000                0.000000           0.000000   \n",
       "50%       10.000000                1.000000           1.000000   \n",
       "75%       11.000000                3.000000           2.000000   \n",
       "max       22.000000              420.000000         360.000000   \n",
       "\n",
       "       Lang_exposition  Section_renforcee  \n",
       "count      1045.000000        1045.000000  \n",
       "mean          5.623541           0.371292  \n",
       "std           9.175903           0.483382  \n",
       "min           0.000000           0.000000  \n",
       "25%           1.000000           0.000000  \n",
       "50%           3.000000           0.000000  \n",
       "75%           6.000000           1.000000  \n",
       "max         120.000000           1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b0d5f0-7515-4759-ac58-aa80b3655b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pseudo</th>\n",
       "      <th>Voc_range</th>\n",
       "      <th>CECRL</th>\n",
       "      <th>L1</th>\n",
       "      <th>Domaine_de_specialite</th>\n",
       "      <th>L2</th>\n",
       "      <th>Note_dialang_ecrit</th>\n",
       "      <th>Lecture_regularite</th>\n",
       "      <th>autre_langue</th>\n",
       "      <th>tache_ecrit</th>\n",
       "      <th>Texte_etudiant</th>\n",
       "      <th>Date_ajout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1045</td>\n",
       "      <td>1015</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>673</td>\n",
       "      <td>992</td>\n",
       "      <td>610</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1015</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1013</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1f2dd72bc867a07a8a11579ddd2345b3d9851622289c5c...</td>\n",
       "      <td>B1</td>\n",
       "      <td>B1</td>\n",
       "      <td>French</td>\n",
       "      <td>Information Communication</td>\n",
       "      <td>Anglais</td>\n",
       "      <td>B1</td>\n",
       "      <td>hebdomadaire</td>\n",
       "      <td>Espagnol</td>\n",
       "      <td>Tache_1</td>\n",
       "      <td>I did'nt know what to do in my life until the ...</td>\n",
       "      <td>2022-09-14 14:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>374</td>\n",
       "      <td>358</td>\n",
       "      <td>999</td>\n",
       "      <td>408</td>\n",
       "      <td>1045</td>\n",
       "      <td>263</td>\n",
       "      <td>262</td>\n",
       "      <td>306</td>\n",
       "      <td>664</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   pseudo Voc_range CECRL  \\\n",
       "count                                                1045      1015  1045   \n",
       "unique                                               1015         6     6   \n",
       "top     1f2dd72bc867a07a8a11579ddd2345b3d9851622289c5c...        B1    B1   \n",
       "freq                                                    2       374   358   \n",
       "\n",
       "            L1      Domaine_de_specialite       L2 Note_dialang_ecrit  \\\n",
       "count     1045                       1045     1045                673   \n",
       "unique      14                         12        1                  6   \n",
       "top     French  Information Communication  Anglais                 B1   \n",
       "freq       999                        408     1045                263   \n",
       "\n",
       "       Lecture_regularite autre_langue tache_ecrit  \\\n",
       "count                 992          610        1045   \n",
       "unique                  4            8           3   \n",
       "top          hebdomadaire     Espagnol     Tache_1   \n",
       "freq                  262          306         664   \n",
       "\n",
       "                                           Texte_etudiant           Date_ajout  \n",
       "count                                                1045                 1045  \n",
       "unique                                               1013                  505  \n",
       "top     I did'nt know what to do in my life until the ...  2022-09-14 14:43:00  \n",
       "freq                                                    2                   24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383b022c-e994-4287-b2f0-80e8ff927c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CECRL\n",
       "B1    358\n",
       "A2    324\n",
       "B2    212\n",
       "A1     90\n",
       "C1     53\n",
       "C2      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"CECRL\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e08b1a2-b954-443f-a0bf-12d6df117fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pseudo                      0\n",
       "Voc_range                  30\n",
       "CECRL                       0\n",
       "nb_annees_L2                0\n",
       "L1                          0\n",
       "Domaine_de_specialite       0\n",
       "Sejours_duree_semaines      0\n",
       "Sejours_frequence           0\n",
       "Lang_exposition             0\n",
       "L2                          0\n",
       "Note_dialang_ecrit        372\n",
       "Lecture_regularite         53\n",
       "autre_langue              435\n",
       "tache_ecrit                 0\n",
       "Texte_etudiant              0\n",
       "Date_ajout                  0\n",
       "Section_renforcee           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c78e331-0c71-4d98-8f2b-7ce758676a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1\n",
       "French                  0.955981\n",
       "Arabic                  0.014354\n",
       "Other                   0.008612\n",
       "English                 0.004785\n",
       "Vietnamese              0.003828\n",
       "Turkish                 0.001914\n",
       "Malagasy                0.001914\n",
       "English##Gan Chinese    0.001914\n",
       "Spanish                 0.001914\n",
       "Italian                 0.000957\n",
       "French##Spanish         0.000957\n",
       "Mandarin                0.000957\n",
       "Amharic                 0.000957\n",
       "Russian                 0.000957\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"L1\"].value_counts()/len(df[\"L1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec680090-5945-425f-8c86-b15e61438482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t\n",
      "\u000b",
      "\f",
      ".()\n"
     ]
    }
   ],
   "source": [
    "map_ = {\n",
    "    c: \" \" for c in string.whitespace\n",
    "}\n",
    "replaces_ = {\n",
    "    \n",
    "    \"...\":\"\",\n",
    "    \"..\": \"\",\n",
    "}\n",
    "specials = string.whitespace + \".()\"\n",
    "print(specials)\n",
    "texts = df[\"Texte_etudiant\"].apply(lambda x: \"\".join([map_.get(ch.lower(),ch.lower()) for ch in x if ch.isalnum() or ch in specials]))\n",
    "texts = texts.apply(lambda x: re.sub(r'\\.{2,}', '', x))\n",
    "texts = texts.apply(lambda x: re.sub(r' {2,}', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4fe310-9292-476b-b518-63bec4a45d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Formatter',\n",
       " 'Template',\n",
       " '_ChainMap',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_re',\n",
       " '_sentinel_dict',\n",
       " '_string',\n",
       " 'ascii_letters',\n",
       " 'ascii_lowercase',\n",
       " 'ascii_uppercase',\n",
       " 'capwords',\n",
       " 'digits',\n",
       " 'hexdigits',\n",
       " 'octdigits',\n",
       " 'printable',\n",
       " 'punctuation',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "dir(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82875a65-cfe4-4510-963d-a564935d50eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9810a8-5fb1-47fd-a083-bb39515bd8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e0d922-1011-4bdb-b6a6-e28d3eb62d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ = \"!#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f3cb9b-70fd-4738-941c-456c9fcd9724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53441c48-f4f0-4fcf-b845-77bd6260654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Formatter',\n",
       " 'Template',\n",
       " '_ChainMap',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_re',\n",
       " '_sentinel_dict',\n",
       " '_string',\n",
       " 'ascii_letters',\n",
       " 'ascii_lowercase',\n",
       " 'ascii_uppercase',\n",
       " 'capwords',\n",
       " 'digits',\n",
       " 'hexdigits',\n",
       " 'octdigits',\n",
       " 'printable',\n",
       " 'punctuation',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73ca25a-8935-47af-a609-2e1d08090269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['being in a earth sciences domain at the beginning of the year my class and i went to a little city called paris un the south of paris', ' after a few classes of cartographic basics it was time for us to do it in real life to see what it is like to be on the field', 'the aim of this trip was to recognize a couple of series of rock and identify which ones belongs to which period of the earth time', ' and then being able to report everything we saw on a map and create a geological map of a place weve never been before', 'for 10 days we had to wake up every morning a 7 to be ready at 8 to walk through the fauna and flora climbing mountains against the wind or the sun or the rain and walk around 5 to 10 kilometers per day until 6pm', ' the field was around 12m2 and rocks we studied about were sedimentary', 'all of us geosciences students were separate into little groups of 2 people', ' my partner and i were watching out and paying attention to all the rocks around us', ' when there is ain placeserie we had to take apendagewhich means the angle of where the serie is directed compared the geographical north pole', ' by taking all thosependageswe noticed that from the north of the field to the average center the series were making a syncline', ' heres our first element of our mapan syncline with all the younger rocks in the center and symmetrically going to the oldest', 'we tried to understand the south of the field but failed it was too complicated for us debutants in cartographics', ' after finishing our map we learnt that the south were in discordance which means news deposits were placed into the olders rocks eroded', 'after all of that we went back to our homes tired of walking and climbing and thinking about what happened in the south', 'cool']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/p/garbage_collector/cefr-classifcation/cefr_classification/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/p/garbage_collector/cefr-classifcation/cefr_classification/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for t in texts:\n",
    "    print(t.split(\".\"))\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c115699-db76-490b-89f7-62d7c9840545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=\"d f\\nd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ecf4341-b0fe-4e79-bcc9-b346b2605ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', ' ', 'f', ' ', 'd']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[map_.get(c, c) for c in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7bed909-d2ff-458c-95ed-774d05942201",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"... . .. .. ....... . .......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a414d161-1ff9-415f-8272-1c40739004c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p,r in replaces_.items():\n",
    "    a = a.replace(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dfcab6e-2eb9-4795-8b79-4833f833a8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' .   . . .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69dd4d15-8847-4f7a-89d6-d68e1a60bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    . \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "a = \"... . .. .. ....... . .......\"\n",
    "\n",
    "# Replace occurrences of more than one dot with a single dot\n",
    "result = re.sub(r'\\.{2,}', '', a)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce35b684-6b41-4037-b232-683978d8f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['being in a earth sciences domain at the beginning of the year my class and i went to a little city called paris un the south of paris',\n",
       " ' after a few classes of cartographic basics it was time for us to do it in real life to see what it is like to be on the field',\n",
       " 'the aim of this trip was to recognize a couple of series of rock and identify which ones belongs to which period of the earth time',\n",
       " ' and then being able to report everything we saw on a map and create a geological map of a place weve never been before',\n",
       " 'for 10 days we had to wake up every morning a 7 to be ready at 8 to walk through the fauna and flora climbing mountains against the wind or the sun or the rain and walk around 5 to 10 kilometers per day until 6pm',\n",
       " ' the field was around 12m2 and rocks we studied about were sedimentary',\n",
       " 'all of us geosciences students were separate into little groups of 2 people',\n",
       " ' my partner and i were watching out and paying attention to all the rocks around us',\n",
       " ' when there is ain placeserie we had to take apendagewhich means the angle of where the serie is directed compared the geographical north pole',\n",
       " ' by taking all thosependageswe noticed that from the north of the field to the average center the series were making a syncline',\n",
       " ' heres our first element of our mapan syncline with all the younger rocks in the center and symmetrically going to the oldest',\n",
       " 'we tried to understand the south of the field but failed it was too complicated for us debutants in cartographics',\n",
       " ' after finishing our map we learnt that the south were in discordance which means news deposits were placed into the olders rocks eroded',\n",
       " 'after all of that we went back to our homes tired of walking and climbing and thinking about what happened in the south',\n",
       " 'cool']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.apply(lambda x: x.split(\".\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3513a022-d26f-425d-8f88-87b9cf47f3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Being in a earth sciences domain, at the beginning of the year my class and I went to a little city called Paris un the south of Paris. After a few classes of cartographic basics, it was time for us to do it in real life, to see what it is like to be on the field.\\n The aim of this trip was to recognize a couple of series of rock, and identify which ones belongs to which period of the earth time. And then, being able to report everything we saw on a map, and create a geological map of a place we've never been before.\\n For 10 days, we had to wake up every morning a 7 to be ready at 8 to walk through the fauna and flora, climbing mountains, against the wind or the sun or the rain, and walk around 5 to 10 kilometers per day, until 6pm. The field was around 12m^2 and rocks we studied about were sedimentary.\\n All of us, geosciences students, were separate into little groups of 2 people. My partner and I were watching out and paying attention to all the rocks around us. When there is a  in place  serie, we had to take a  pendage  which means the angle of where the serie is directed compared the geographical north pole. By taking all those  pendages , we noticed that, from the north of the field to the average center, the series were making a syncline. Heres our first element of our map ! An syncline, with all the younger rocks in the center and, symmetrically, going to the oldest.\\n We tried to understand the south of the field but failed, it was too complicated for us, debutants in cartographics. After finishing our map, we learnt that the south were in discordance, which means news deposits were placed into the olders rocks eroded.\\n After all of that, we went back to our homes, tired of walking and climbing and thinking about what happened in the south.\\n cool\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Texte_etudiant\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca661fb9-ef19-4c48-a829-7a636a666ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262dc20-0398-4764-aacd-abebe8f693a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065b99b-afbc-4f7e-931d-cf07eeff9847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc350d9-fd7d-402d-b2d5-cd7c02a910ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbe6fd-0d5c-4e5d-bfa3-c7f845cec4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
